<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CompBench: Benchmarking Complex Instruction-guided Image Editing</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CompBench: Benchmarking Complex Instruction-guided Image Editing</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Bohan Jia</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Wenxuan Huang</a><sup>1,2*</sup>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yuntian Tang</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Junbo Qiao</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jincheng Liao</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Shaosheng Cao</a><sup>3&#9993;</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Fei Zhao</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhaopeng Feng</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhouhong Gu</a><sup>5</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhenfei Yin</a><sup>6</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Lei Bai</a><sup>7</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Wanli Ouyang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Lin Chen</a><sup>8</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Fei Zhao</a><sup>9</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zihan Wang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yuan Xie</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Shaohui Lin</a><sup>1&#9993;</sup>
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <sup>1</sup>East China Normal University&nbsp;&nbsp;
                    <sup>2</sup>The Chinese University of Hong Kong<br>
                    <sup>3</sup>Xiaohongshu Inc.&nbsp;&nbsp;
                    <sup>4</sup>Zhejiang University&nbsp;&nbsp;
                    <sup>5</sup>Fudan University&nbsp;&nbsp;
                    <sup>6</sup>University of Oxford<br>
                    <sup>7</sup>Shanghai Jiao Tong University&nbsp;&nbsp;
                    <sup>8</sup>University of Science and Technology of China&nbsp;&nbsp;
                    <sup>9</sup>Nanjing University
                    <div class="eql-cntrb"><sup>*</sup>Indicates Equal Contribution</div>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2505.12200" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Hugginface link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/BohanJia/CompBench" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1.2em;">
                      </span>
                      <span>Datasets</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/BhJia/ComBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

               
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While real-world applications increasingly demand intricate scene manipulation, existing instruction-guided image editing benchmarks often oversimplify task complexity and instruction comprehensiveness. To address this gap, we introduce Comp-Edit, a large-scale benchmark specifically designed for complex instruction-guided image editing. Our Comp-Edit contains complicated tasks requiring fine-grained instruction-following, spatial-contextual reasoning and precise editing capabilities of image editing model. Meanwhile, To better align instructions with complex editing requirements, we propose an instruction decoupling method that disentangles editing intents into four dimensions: location (spatial constraints), appearance (visual attributes), dynamics (temporal interactions), and objects (entity relationships). Through extensive evaluations, we demonstrate that Comp-Edit exposes fundamental limitations in current methods, which offers a critical tool for advancing next-generation image editing models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero teaser">
  <!-- <section class="section"> -->
    <div class="container is-max-desktop">
      <!-- <div class="columns is-centered has-text-centered">-->
      <!-- <div class="column is-four-fifths"> -->
          <img src="static/images/teaser.png">
          <h3 class="subtitle has-text-centered">
            <small>
            <strong>CompBench</strong> offers diverse instruction-guided image editing tasks across nine categories: object addition, object removal, object replacement, multi-object editing, multi-turn editing, implicit reasoning, action editing, location editing and viewpoint editing
            </small>
          </h3>
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2> <br>

        <div class="content has-text-justified">
        <p>
          Our complex instruction-guided image editing benchmark, <strong>CompBench</strong>, contains 3k+ image-instruction pairs. To enhance the comprehensiveness of evaluation, we categorize editing tasks into <strong>five major classes</strong> with <strong>specific tasks</strong> based on their characteristics:(1) Local Editing: focuses on manipulating local objects, including object removal, object addition and object replacement. (2) Multi-editing: addresses interactions among multiple objects or editing steps, including multi-turn editing and multi-object editing. (3) Action Editing: modifies the dynamic states or interactions of objects. (4) Scene Spatial Editing: alters scene spatial properties, consisting of location editing and viewpoint editing. (5) Complex Reasoning: requires implicit logical reasoning, including implicit reasoning.        </p>
        <div class="columns is-centered">
          <div class="column is-half has-text-centered">
            <img src="static/images/tasks/tasks-1.png" alt="Tasks Image">
          </div>
          <div class="column is-half has-text-centered">
            <img src="static/images/model_ssim_comparison_bubble.png" alt="Another Image">
          </div>
        </div>
        <p>
          Every sample in CompBench is meticulously constructed through multiple rounds of expert review, ensuring the highest quality of edits. Unlike other benchmarks where editing failures are common, all data in CompBench represent successfully executed editing results, with SSIM (Structural Similarity Index Measure) scores significantly outperforming those of other datasets. This rigorous quality control ensures that CompBench provides a reliable assessment of model performance in realistically complex editing scenarios.
        </p>
        <img src="static/images/Comparison.jpg">
      </div>
    </div>
    <!--/ Dataset Overview. -->
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Curation Pipeline. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Data Curation</h2> <br>
  
          <div class="content has-text-justified">
            <p>
              The pipeline consists of two main stages: (a) Source data collection and preprocessing, wherein high-quality data are identified through image quality filtering, mask decomposition, occlusion and continuity evaluation, followed by thorough human verification. (b) Task-specific data generation using four specialized pipelines within our MLLM-Human Collaborative Framework, where multimodal large language models generate initial editing instructions that are subsequently validated by humans to ensure high-fidelity, semantically aligned instruction-image pairs for complex editing tasks.
            </p>
  
          </div>
          <img src="static/images/Overall/Overall-1.png">
          <!--
          <h3 class="subtitle has-text-centered">
            <br>
            <small>CompBench Curation Pipeline.</small>
          </h3>-->
          
  
          
        </div>
      </div>
      <!--/ Curation Pipeline. -->
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Results. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2> <br>
  

          <div class="content has-text-justified">
            <p>
              We evaluate instruction-guided image editing models only, including InstructPix2pix, MagicBrush, HIVE, MGIE, and others. Evaluation covers foreground accuracy and background consistency, using PSNR, SSIM, LPIPS, and CLIP-based metrics. For tasks with major object changes (e.g., action or viewpoint), we use GPT-4o with tailored prompts to score edits from 0 to 10.
            </p>

          </div>
          
          <div class="content has-text-justified">
            <p>
              LC-T denotes local CLIP scores between the edited foreground and the local description. LC-I refers to the CLIP image similarity between the foreground edited result and ground truth (GT) image. Top-three evaluation results are highlighted in  (1st), blue(2nd), and green (3rd).            </p>
          </div>
  
          <div class="content has-text-justified">
            
          </div>
          
        </div>
      </div>
      
      <!--/ Results. -->
    </div>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/multi-turn.jpg" alt="MY ALT TEXT" style="width: 800px; height: auto; display: block; margin: 0 auto;"/>
        <h2 class="subtitle has-text-centered">
          Evaluation results on multi-turn editing.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/act_loc_view.jpg" alt="MY ALT TEXT" style="width: 600px; height: auto; display: block; margin: 0 auto;"/>
        <h2 class="subtitle has-text-centered">
         Evaluation results on action editing, location editing and viewpoint editing tasks
       </h2>
     </div>
     <div class="item">
        <!-- Your image here -->
        <img src="static/images/local_editing.jpg" alt="MY ALT TEXT" />
        <h2 class="subtitle has-text-centered">
          Evaluation results on local editing, multi-object editing and implicit reasoning
        </h2>
      </div>
  </div>
</div>
</div>
</section>

</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/case1/case1-1.png" alt="MY ALT TEXT" style="width: 800px; height: auto; display: block; margin: 0 auto;"/>
        <h2 class="subtitle has-text-centered">
          Cases of Local Editing Results
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/case2/case2-1.png" alt="MY ALT TEXT" style="width: 700px; height: auto; display: block; margin: 0 auto;"/>
        <h2 class="subtitle has-text-centered">
         More Cases of Local Editing Results
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/model_act_loc_view_1/model_act_loc_view_1-1.png" style="width: 500px; height: auto; display: block; margin: 0 auto;"/>
      <h2 class="subtitle has-text-centered">
        Action, Location and Viewpoint Editing Examples
      </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/model_act_loc_view_2/model_act_loc_view_2-1.png" style="width: 650px; height: auto; display: block; margin: 0 auto;"/>
      <h2 class="subtitle has-text-centered">
        Action, Location and Viewpoint Editing Examples
      </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/model_act_loc_view_3/model_act_loc_view_3-1.png" style="width: 650px; height: auto; display: block; margin: 0 auto;"/>
      <h2 class="subtitle has-text-centered">
        Action, Location and Viewpoint Editing Examples
      </h2>
     </div>
  </div>
</div>
</div>
</section>



<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
